# Hierarchical Multimodal Fusion for Whole Slide Image Analysis

In recent years, deep learning has significantly advanced histopathology whole slide image (WSI) analysis. However, current models are often based on Multiple Instance Learning (MIL), which mainly focus on isolated slides and lack a comprehensive patient-level representation.

In this project, we propose a **novel multi-modal hierarchical (MH) framework** that integrates WSI images and textual data through a **Bi-Cross Attention (BCA)** mechanism to achieve explicit cross-modal alignment and enable richer patient-level representations.

Unlike conventional slide-level approaches, our hierarchical model aggregates information from multiple slides per patient, yielding more reliable cancer diagnoses.

Extensive experiments on both slide-level and patient-level pathology classification tasks demonstrate that our proposed method **significantly outperforms state-of-the-art** models. The integration of multi-modal cues guided by text prompts enhances the discriminative power of WSI representations, while hierarchical aggregation further boosts diagnostic confidence.

These results highlight the benefit of combining visual and textual modalities in a unified framework, demonstrating the potential for broader applications in multi-modal biomedical analysis and general computational pathology.

---

## üîç Keywords
- Whole slide image analysis  
- Multi-modal learning  
- Cross-modal attention  
- Hierarchical classification  
- Multiple instance learning  
- Computational pathology

---

## üìÅ Project Structure

